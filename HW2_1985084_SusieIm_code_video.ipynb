{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2d9fab",
   "metadata": {},
   "source": [
    "# HW 2\n",
    "## 학과: 경영학부\n",
    "## 학번: 1985084\n",
    "## 이름: 임수지\n",
    "## 영상 링크: https://youtu.be/kRkHcgeVAsg?si=zU3BXCxJ6PQi5Zs-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eea370",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "2차원 torch 배열(텐서)을 입력받아, 각 **행(row)** 의 합을 계산하여 리스트로 반환하는 함수를 작성하시오.  \n",
    "반드시 **`for` 반복문**을 사용하여 합을 구해야 하며, `torch.sum()` 등의 내장 합산 함수를 사용해서는 안 된다.\n",
    "\n",
    "#### 힌트\n",
    "- `for row in A:` 구문으로 각 행을 순회할 수 있다.  \n",
    "- `append()`를 이용해 리스트에 원소를 추가할 수 있다.  \n",
    "- `element.item()`을 사용하면 tensor 형태의 값을 일반 숫자로 변환할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0c2e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def row_sums_loop(A):\n",
    "    \"\"\"\n",
    "    2차원 torch 텐서를 입력받아, 각 행의 합을 리스트로 반환하는 함수.\n",
    "    반드시 for문을 사용하여 직접 계산해야 함.\n",
    "\n",
    "    매개변수:\n",
    "        A (torch.Tensor): (행, 열) 형태의 2차원 텐서\n",
    "\n",
    "    반환값:\n",
    "        list: 각 행의 합을 담은 리스트 (일반 숫자 형태)\n",
    "    \"\"\"\n",
    "    row_sums = []  # 행의 합을 저장할 리스트\n",
    "    \n",
    "    # 각 행을 순회\n",
    "    for row in A:\n",
    "        total = 0  # 행의 합을 저장할 변수 초기화\n",
    "        \n",
    "        # 행의 각 원소를 순회하면서 합계 계산\n",
    "        for element in row:\n",
    "            total += element.item()  # tensor 값을 일반 숫자로 변환하여 더하기\n",
    "        \n",
    "        row_sums.append(total)  # 계산한 합을 리스트에 추가\n",
    "    \n",
    "    return row_sums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f938571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텐서:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "행 합계 결과: [6, 15]\n"
     ]
    }
   ],
   "source": [
    "# ✅ 예시 실행\n",
    "A = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "result = row_sums_loop(A)\n",
    "print(\"입력 텐서:\\n\", A)\n",
    "print(\"행 합계 결과:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafdc21d",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "이 문제는 **반복문(for loop)** 을 이용해 2차원 텐서의 각 행을 순회하며  \n",
    "그 안의 원소들을 하나씩 더해 나가는 기본적인 연습문제입니다.\n",
    "\n",
    "1. `for row in A:`  \n",
    "   → 2차원 텐서의 각 행을 하나씩 꺼냅니다.  \n",
    "     예를 들어 `A`가  \n",
    "     \\[\n",
    "     \\begin{bmatrix}1 & 2 & 3\\\\ 4 & 5 & 6\\end{bmatrix}\n",
    "     \\]\n",
    "     이라면, 첫 번째 반복에서는 `[1, 2, 3]`, 두 번째 반복에서는 `[4, 5, 6]`이 `row`에 들어옵니다.\n",
    "\n",
    "2. `for element in row:`  \n",
    "   → 각 행 안의 원소를 하나씩 꺼내어 합을 계산합니다.  \n",
    "     이때 `element`는 `tensor(3)`과 같은 **0차원 텐서(스칼라 텐서)** 형태이므로,  \n",
    "     `.item()`을 이용해 일반 숫자로 바꾸어야 합니다.  \n",
    "\n",
    "3. `total += element.item()`  \n",
    "   → 변환된 숫자를 `total` 변수에 더합니다.\n",
    "\n",
    "4. `row_sums.append(total)`  \n",
    "   → 한 행의 합이 완성되면 이를 리스트에 추가합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18528978",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "2차원 torch 배열(텐서)을 입력받아, **중첩된 for문(nested loops)** 을 이용해  \n",
    "행과 열을 바꾸어 놓은 **전치 행렬(transpose)** 을 직접 계산하는 함수를 작성하시오.  \n",
    "(즉, 내장 함수 `A.T`나 `torch.transpose()`를 사용하지 말고,  \n",
    "각 원소를 하나씩 옮겨서 새로운 텐서를 만들어야 한다.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "030b9c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def transpose_loop(A):\n",
    "    \"\"\"\n",
    "    2차원 torch 텐서 A의 전치행렬(transpose)을\n",
    "    중첩된 for문을 이용해 직접 계산하는 함수.\n",
    "\n",
    "    매개변수:\n",
    "        A (torch.Tensor): (행, 열) 형태의 2차원 텐서\n",
    "    \n",
    "    반환값:\n",
    "        torch.Tensor: A의 전치행렬 (열, 행) 형태\n",
    "    \"\"\"\n",
    "    nrow, ncol = A.shape  # 원래 행과 열의 크기\n",
    "    # 전치행렬은 (열, 행) 형태이므로 반대로 초기화\n",
    "    A_T = torch.zeros((ncol, nrow), dtype=A.dtype)\n",
    "\n",
    "    # 행, 열을 순회하며 위치를 바꾸어 채워 넣기\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            A_T[j, i] = A[i, j]\n",
    "    \n",
    "    return A_T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "deb087b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 텐서:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "전치행렬 결과:\n",
      " tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "# ✅ 예시 실행\n",
    "A = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "result = transpose_loop(A)\n",
    "print(\"입력 텐서:\\n\", A)\n",
    "print(\"전치행렬 결과:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafcbafc",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "이 문제는 **행(row)과 열(column)을 뒤바꾸는 전치 연산(transpose)** 을  \n",
    "반복문으로 직접 구현하는 연습문제입니다.\n",
    "\n",
    "1. `nrow, ncol = A.shape`  \n",
    "   → 행렬의 행 개수와 열 개수를 구함.  \n",
    "2. `A_T = torch.zeros((ncol, nrow))`  \n",
    "   → 전치된 형태로 저장할 새로운 텐서를 0으로 초기화.  \n",
    "     (전치행렬의 크기는 기존의 (열, 행) 크기임)  \n",
    "3. 두 개의 for문을 이용하여 원소를 하나씩 옮김:\n",
    "여기서 A[i, j]의 값은 전치행렬의 [j, i] 위치로 이동함.\n",
    "4. 완성된 전치행렬 A_T를 반환함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145bfa8b",
   "metadata": {},
   "source": [
    "### Problem 3 https://youtu.be/kRkHcgeVAsg?si=zU3BXCxJ6PQi5Zs- 영상 0:00~1:43\n",
    "\n",
    "1차원 torch 텐서를 입력받아 **Run-Length Encoding (RLE)** 을 수행하는 함수를 작성하시오.  \n",
    "RLE는 연속해서 같은 값이 반복될 때,  \n",
    "그 값을 한 번만 기록하고 반복 횟수를 함께 저장하는 압축 방식이다.\n",
    "\n",
    "함수는 두 개의 Python **리스트**를 반환해야 한다:\n",
    "- `values`: 연속 구간의 고유 값들\n",
    "- `counts`: 해당 값이 연속된 횟수들  \n",
    "\n",
    "반환되는 리스트에는 torch 텐서가 아닌 **일반 파이썬 숫자**만 포함되어야 한다.  \n",
    "(`tensor(3)`을 `3`으로 바꾸려면 `.item()`을 사용)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b235673",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def run_length_encode(seq):\n",
    "    \"\"\"\n",
    "    1차원 torch 텐서에 대해 Run-Length Encoding(RLE)을 수행하는 함수.\n",
    "\n",
    "    매개변수:\n",
    "        seq (torch.Tensor): 1차원 텐서 (예: tensor([1,1,2,2,3]))\n",
    "\n",
    "    반환값:\n",
    "        values (list): 연속 구간의 값들을 담은 리스트 (파이썬 숫자)\n",
    "        counts (list): 각 값의 반복 횟수를 담은 리스트\n",
    "    \"\"\"\n",
    "    values = []  # 고유 값 저장\n",
    "    counts = []  # 각 값의 반복 횟수 저장\n",
    "\n",
    "    if len(seq) == 0:\n",
    "        return values, counts  # 빈 텐서일 경우 빈 리스트 반환\n",
    "\n",
    "    # 첫 번째 원소를 기준으로 시작\n",
    "    current_value = seq[0].item()\n",
    "    count = 1\n",
    "\n",
    "    # 두 번째 원소부터 끝까지 순회\n",
    "    for i in range(1, len(seq)):\n",
    "        val = seq[i].item()\n",
    "        if val == current_value:\n",
    "            count += 1  # 같은 값이면 카운트 증가\n",
    "        else:\n",
    "            # 값이 바뀌면 저장 후 초기화\n",
    "            values.append(current_value)\n",
    "            counts.append(count)\n",
    "            current_value = val\n",
    "            count = 1\n",
    "\n",
    "    # 마지막 구간도 저장\n",
    "    values.append(current_value)\n",
    "    counts.append(count)\n",
    "\n",
    "    return values, counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69aca0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 시퀀스: [1, 1, 1, 2, 2, 3, 3, 3, 3, 2]\n",
      "values: [1, 2, 3, 2]\n",
      "counts: [3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "# ✅ 예시 실행\n",
    "seq = torch.tensor([1,1,1,2,2,3,3,3,3,2])\n",
    "values, counts = run_length_encode(seq)\n",
    "\n",
    "print(\"입력 시퀀스:\", seq.tolist())\n",
    "print(\"values:\", values)\n",
    "print(\"counts:\", counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5747c6bb",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "이 문제는 **반복문과 조건문을 이용한 기본 알고리즘 구현 능력**을 평가하는 문제입니다.  \n",
    "`torch`의 고급 함수 대신 순수한 논리 흐름으로 문제를 해결해야 합니다.\n",
    "\n",
    "1. 초기화\n",
    "→ 첫 번째 원소를 현재 구간의 값으로 설정하고 카운트를 1로 시작합니다.\n",
    "\n",
    "2. 반복문으로 순회\n",
    "→ 이전 값과 같으면 카운트를 늘리고,\n",
    "다르면 기존 구간을 리스트에 저장하고 새 구간을 시작합니다.\n",
    "\n",
    "3. 마지막 구간 저장\n",
    "4. 결과 반환\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5474c4",
   "metadata": {},
   "source": [
    "### Problem 4 https://youtu.be/kRkHcgeVAsg?si=zU3BXCxJ6PQi5Zs- 영상 1:48~3:31\n",
    "\n",
    "PyTorch에서 `backward()`를 같은 변수에 여러 번 호출하면,  \n",
    "이전의 gradient 값이 **누적(accumulate)** 되어 `x.grad`에 더해집니다.  \n",
    "즉, 수학적으로는 새로운 도함수를 구하는 것처럼 보이지만,  \n",
    "PyTorch에서는 기본적으로 **기존의 gradient에 더하는 방식**으로 동작합니다.\n",
    "\n",
    "따라서 새로운 gradient를 처음부터 다시 계산하고 싶다면,  \n",
    "반드시 `x.grad.zero_()`로 기존 값을 **초기화(reset)** 해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6f6f0b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 backward 후: tensor([4.])\n",
      "두 번째 backward 후: tensor([8.])\n",
      "zero_() 후 backward: tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# (i) x 정의 및 z = x**2 계산\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "z = x**2\n",
    "\n",
    "# (ii) 첫 번째 backward 호출\n",
    "z.backward()\n",
    "print(\"첫 번째 backward 후:\", x.grad)\n",
    "\n",
    "# (iii) 기존 grad를 초기화하지 않고 다시 backward 호출\n",
    "z = x**2\n",
    "z.backward()\n",
    "print(\"두 번째 backward 후:\", x.grad)\n",
    "\n",
    "# (iv) grad 초기화 후 다시 계산\n",
    "x.grad.zero_()  # 기존 gradient 제거\n",
    "z = x**2\n",
    "z.backward()\n",
    "print(\"zero_() 후 backward:\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20330b6b",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "| 단계 | 코드 요약 | 기대 출력 | 설명 |\n",
    "|:--|:--|:--|:--|\n",
    "| (i) | `x = torch.tensor([2.0], requires_grad=True)` <br> `z = x**2` | - | 학습 가능한 변수 x 정의 |\n",
    "| (ii) | `z.backward()` | `x.grad = tensor([4.])` | \\(\\frac{dz}{dx} = 2x = 4\\) |\n",
    "| (iii) | `z = x**2; z.backward()` (초기화 안 함) | `x.grad = tensor([8.])` | 이전 4에 새 4가 더해져 총 8 |\n",
    "| (iv) | `x.grad.zero_()` 후 다시 `z.backward()` | `x.grad = tensor([4.])` | 초기화 후 새 gradient만 반영됨 |\n",
    "\n",
    "---\n",
    "\n",
    "#### 2️⃣ 왜 두 번째 backward 결과가 더 큰가?\n",
    "\n",
    "PyTorch에서는 `backward()` 호출 시 기존 gradient를 지우지 않고 **누적(add)** 하기 때문입니다.  \n",
    "그래서 첫 번째 backward에서 얻은 값 4가 그대로 남아 있고,  \n",
    "두 번째 backward에서 새로 계산된 4가 더해져 최종적으로 8이 됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3️⃣ `zero_()`를 이용한 초기화\n",
    "\n",
    "`x.grad.zero_()`는 `x.grad` 텐서의 값을 0으로 바꿉니다.  \n",
    "이후 `backward()`를 호출하면 새로운 gradient만 저장됩니다.  \n",
    "이 방식은 “기존의 gradient는 버리고 새로 시작”할 때 필요합니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4️⃣ (v) PyTorch가 기본적으로 gradient를 누적하는 이유\n",
    "\n",
    "PyTorch는 **mini-batch 학습(batch training)** 을 지원하기 위해  \n",
    "gradient를 기본적으로 **누적(accumulate)** 하는 구조를 가집니다.\n",
    "\n",
    "- 신경망 학습에서는 하나의 batch(예: 데이터 32개)에 대해  \n",
    "  forward → backward를 수행한 후,  \n",
    "  다음 batch의 gradient를 **이전 gradient에 더하는 방식**으로  \n",
    "  전체 데이터의 평균적인 gradient를 계산합니다.\n",
    "- 즉, `x.grad`는 여러 batch의 gradient 합을 유지하게 되므로,  \n",
    "  **전체 데이터셋의 gradient 근사치**를 쉽게 구할 수 있습니다.\n",
    "\n",
    "따라서 PyTorch는 gradient를 덮어쓰지 않고 누적하는 것이 기본 동작이며,  \n",
    "이로 인해 개발자는 여러 batch의 기울기를 직접 합산할 필요가 없습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47069373",
   "metadata": {},
   "source": [
    "### Problem 5 https://youtu.be/kRkHcgeVAsg?si=zU3BXCxJ6PQi5Zs- 영상 3:32~6:05\n",
    "\n",
    "다음과 같이 정의하자.\n",
    "\\[\n",
    "x = 2.0,\\quad\n",
    "y = x^3,\\quad\n",
    "z = 2y + \\sin(y)\n",
    "\\]\n",
    "\n",
    "PyTorch에서 `requires_grad=True`로 텐서를 생성하면,  \n",
    "그 이후 모든 계산(`x**3`, `sin(y)` 등)을 **계산 그래프(computation graph)** 로 기록합니다.  \n",
    "이 그래프를 이용해 `.backward()`를 호출하면,  \n",
    "계산 과정을 역추적하며 자동으로 미분을 수행할 수 있습니다.\n",
    "\n",
    "그러나 이 계산 그래프는 메모리를 많이 차지하므로,  \n",
    "PyTorch는 기본적으로 한 번 `backward()`를 호출하면 그래프를 **자동으로 삭제(free)** 합니다.  \n",
    "만약 동일한 그래프를 다시 사용하고 싶다면,  \n",
    "`retain_graph=True` 옵션을 주어 그래프를 **유지(keep)** 해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f528782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 2.0   y = 8.0   z = 16.98935890197754\n",
      "dz/dx at x=2 : tensor([22.2540])\n",
      "Error message: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# (i) x, y, z 정의\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x**3\n",
    "z = 2*y + torch.sin(y)\n",
    "print(\"x =\", x.item(), \"  y =\", y.item(), \"  z =\", z.item())\n",
    "\n",
    "# (ii) z.backward() 호출 후 x.grad 출력 → dz/dx 계산\n",
    "z.backward()\n",
    "print(\"dz/dx at x=2 :\", x.grad)\n",
    "\n",
    "# (iii) y.backward() 호출 (z.backward 이후)\n",
    "try:\n",
    "    y.backward()\n",
    "except Exception as e:\n",
    "    print(\"Error message:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d0bc1",
   "metadata": {},
   "source": [
    "### 💬 (iii) 해설\n",
    "\n",
    "이때 오류가 발생합니다.\n",
    "\n",
    "> RuntimeError: Trying to backward through the graph a second time...\n",
    "\n",
    "왜냐하면 `z.backward()` 이후 그래프가 이미 메모리에서 해제되었기 때문입니다.  \n",
    "따라서 같은 계산 그래프를 사용해 다시 `backward()`를 호출할 수 없습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de169c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx at x=2 : tensor([22.2540])\n",
      "dy/dx at x=2 : tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "# (iv) retain_graph=True 사용 예시\n",
    "import torch\n",
    "\n",
    "# 계산 그래프 다시 생성\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x**3\n",
    "z = 2*y + torch.sin(y)\n",
    "\n",
    "# (A) 그래프를 유지하면서 z.backward() 실행\n",
    "z.backward(retain_graph=True)\n",
    "print(\"dz/dx at x=2 :\", x.grad)\n",
    "\n",
    "# (B) 기존 gradient 제거\n",
    "x.grad.zero_()\n",
    "\n",
    "# (C) 같은 그래프에서 y.backward() 실행 → dy/dx 계산\n",
    "y.backward()\n",
    "print(\"dy/dx at x=2 :\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe0c82",
   "metadata": {},
   "source": [
    "### 💬 (iv) 해설\n",
    "\n",
    "| 코드 구간 | 설명 |\n",
    "|------------|------|\n",
    "| (A) `retain_graph=True` | 첫 번째 `backward()` 시 그래프가 해제되지 않도록 유지함. <br>이후 또 다른 `backward()`를 실행할 수 있음. |\n",
    "| (B) `x.grad.zero_()` | 기존의 gradient(이전 `dz/dx` 값)를 0으로 초기화. <br>이 과정을 생략하면 `dy/dx`가 기존 gradient 위에 누적되어 잘못된 값이 나옴. |\n",
    "| (C) `y.backward()` | 같은 그래프를 이용해 `dy/dx` 계산. \\(dy/dx = 3x^2 = 12\\). |\n",
    "\n",
    "만약 (B)의 `x.grad.zero_()`를 생략하면,  \n",
    "기존의 `dz/dx`가 남아 있어 `dy/dx`가 **누적(accumulate)** 되어 더 큰 값이 나옵니다.  \n",
    "따라서 항상 새로운 미분을 계산하기 전에는 `.zero_()`로 초기화해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1b43cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx (fresh graph): tensor([22.2540])\n",
      "dy/dx (fresh graph): tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "# (v) 그래프를 새로 만드는 방법 (rebuild 방식)\n",
    "\n",
    "# Pass 1: dz/dx 계산\n",
    "import torch\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x**3\n",
    "z = 2*y + torch.sin(y)\n",
    "z.backward()\n",
    "print(\"dz/dx (fresh graph):\", x.grad)\n",
    "\n",
    "# Pass 2: 그래프 새로 만들어 dy/dx 계산\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x**3\n",
    "y.backward()\n",
    "print(\"dy/dx (fresh graph):\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31358cc",
   "metadata": {},
   "source": [
    "### 💬 (v) 해설\n",
    "\n",
    "이 방식은 `retain_graph=True`를 쓰지 않고,  \n",
    "**필요할 때마다 계산 그래프를 새로 만드는 방식**입니다.\n",
    "\n",
    "| 코드 구간 | 설명 |\n",
    "|------------|------|\n",
    "| Pass 1 | `dz/dx` 계산을 위한 그래프를 생성 |\n",
    "| Pass 2 | 새로운 `x`와 `y`로 그래프를 다시 만들고 `dy/dx` 계산 |\n",
    "\n",
    "이 방식의 장점은 불필요한 그래프 유지로 인한 메모리 낭비가 없다는 점입니다.  \n",
    "다만, 같은 계산을 여러 번 반복해야 하므로 연산 비용이 늘어날 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f55443ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx (fresh graph): tensor([22.2540])\n",
      "dy/dx (fresh graph): tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "# (vi) zero_()를 이용한 대안적 방식\n",
    "\n",
    "# Pass 1: dz/dx 계산\n",
    "import torch\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x**3\n",
    "z = 2*y + torch.sin(y)\n",
    "z.backward()\n",
    "print(\"dz/dx (fresh graph):\", x.grad)\n",
    "\n",
    "# Pass 2: zero_()로 초기화 후 새 계산\n",
    "x.grad.zero_()\n",
    "y = x**3\n",
    "y.backward()\n",
    "print(\"dy/dx (fresh graph):\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77b2dc1",
   "metadata": {},
   "source": [
    "### 💬 (vi) 해설\n",
    "\n",
    "| 코드 구간 | 설명 |\n",
    "|------------|------|\n",
    "| Pass 1 | `dz/dx` 계산 (새 그래프). |\n",
    "| `x.grad.zero_()` | 기존 gradient 제거. <br>새로운 미분값만 반영되도록 초기화. |\n",
    "| Pass 2 | 새 `y = x**3` 계산 후 `y.backward()` → `dy/dx = 12` |\n",
    "\n",
    "이 방식은 **그래프를 다시 생성하지 않고** 동일한 변수 `x`를 재사용할 수 있다는 장점이 있습니다.  \n",
    "하지만 `y = x**3` 등의 계산을 다시 실행해야 하므로,  \n",
    "결국 그래프를 내부적으로 다시 구성합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58c9b7",
   "metadata": {},
   "source": [
    "### Problem 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d667bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood (누적): -12.272397994995117\n",
      "누적된 gradient theta.grad: tensor([6.])\n",
      "log-likelihood (누적): -24.544795989990234\n",
      "누적된 gradient theta.grad: tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 데이터 및 파라미터 설정\n",
    "X_vals = torch.tensor([1,3,2,5,0, 2,1,3,5,0], dtype=torch.float32)\n",
    "theta  = torch.tensor([1.0], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Dataset / DataLoader (mini-batch 크기 = 5)\n",
    "ds  = TensorDataset(X_vals)\n",
    "ldr = DataLoader(ds, batch_size=5, shuffle=False)\n",
    "\n",
    "total_ll = 0.0  # 전체 로그우도 합\n",
    "\n",
    "for (batch,) in ldr:\n",
    "    # 배치 로그우도 계산\n",
    "    ll_batch = torch.sum(batch * torch.log(theta) - theta - torch.lgamma(batch + 1.0))\n",
    "    total_ll += ll_batch.item()\n",
    "\n",
    "    # backward() 호출 → gradient 누적\n",
    "    ll_batch.backward()\n",
    "    print(\"log-likelihood (누적):\", total_ll)\n",
    "    print(\"누적된 gradient theta.grad:\", theta.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a744f",
   "metadata": {},
   "source": [
    "### (i) 각 반복(iteration)에서의 `ll_batch` 및 `total_ll` 값\n",
    "\n",
    "**첫 번째 batch (X₁~X₅ = [1,3,2,5,0])**\n",
    "\\[\n",
    "\\ell_1(\\theta=1) = \\sum (x_i \\log 1 - 1 - \\log(x_i!))\n",
    "= \\sum (-1 - \\log(x_i!))\n",
    "\\]\n",
    "\n",
    "`log(1)=0`이므로, 단순히 `-1 - log(x_i!)`의 합입니다.  \n",
    "대략적인 값은 다음과 같습니다.\n",
    "\\[\n",
    "\\ell_1(1) \\approx -1 - (0 + 1.099 + 0.693 + 4.787 + 0) = -7.58\n",
    "\\]\n",
    "\n",
    "**두 번째 batch (X₆~X₁₀ = [2,1,3,5,0])**\n",
    "\\[\n",
    "\\ell_2(1) \\approx -1 - (0.693 + 0 + 1.099 + 4.787 + 0) = -7.58\n",
    "\\]\n",
    "\n",
    "즉, `ll_batch` 값은 두 배치 모두 약 `-7.58` 정도이며,  \n",
    "누적합 `total_ll`은 다음과 같이 갱신됩니다.\n",
    "\n",
    "| 반복 | 배치 데이터 | ll_batch | total_ll |\n",
    "|------|--------------|-----------|-----------|\n",
    "| 1회차 | [1,3,2,5,0] | ≈ -7.58 | ≈ -7.58 |\n",
    "| 2회차 | [2,1,3,5,0] | ≈ -7.58 | ≈ -15.16 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b9830",
   "metadata": {},
   "source": [
    "### (ii) 각 반복에서의 `theta.grad` 값\n",
    "\n",
    "점수함수(score function)는  \n",
    "\\[\n",
    "\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}\n",
    "= \\sum_{i=1}^{n} \\left(\\frac{X_i}{\\theta} - 1\\right)\n",
    "\\]\n",
    "\n",
    "따라서 \\(\\theta=1\\)일 때,\n",
    "\n",
    "- **첫 번째 배치**\n",
    "  \\[\n",
    "  \\sum_{i=1}^{5} (X_i - 1)\n",
    "  = (1-1) + (3-1) + (2-1) + (5-1) + (0-1) = 6\n",
    "  \\]\n",
    "  → `theta.grad = tensor([6.])`\n",
    "\n",
    "- **두 번째 배치**\n",
    "  \\[\n",
    "  \\sum_{i=6}^{10} (X_i - 1)\n",
    "  = (2-1) + (1-1) + (3-1) + (5-1) + (0-1) = 6\n",
    "  \\]\n",
    "  PyTorch는 `.backward()`를 호출할 때마다 **gradient를 누적(accumulate)** 하므로  \n",
    "  두 번째 backward 이후에는\n",
    "  \\[\n",
    "  \\text{누적된 } \\theta.grad = 6 + 6 = 12\n",
    "  \\]\n",
    "\n",
    "| 반복 | 배치 데이터 | θ.grad(현재 배치) | θ.grad(누적 결과) |\n",
    "|------|--------------|--------------------|-------------------|\n",
    "| 1회차 | [1,3,2,5,0] | 6 | 6 |\n",
    "| 2회차 | [2,1,3,5,0] | 6 | 12 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c9ab6",
   "metadata": {},
   "source": [
    "### (iii) 전체 데이터에 대한 점수함수 (총 gradient)\n",
    "\n",
    "전체 데이터 10개에 대해,\n",
    "\\[\n",
    "\\left.\\frac{\\partial \\ell(\\theta)}{\\partial \\theta}\\right|_{\\theta=1}\n",
    "= \\sum_{i=1}^{10} (X_i - 1)\n",
    "\\]\n",
    "\\[\n",
    "= (1-1)+(3-1)+(2-1)+(5-1)+(0-1)+(2-1)+(1-1)+(3-1)+(5-1)+(0-1)\n",
    "= 12\n",
    "\\]\n",
    "\n",
    "따라서 코드에서 마지막 출력되는 `theta.grad` 값은\n",
    "\\[\n",
    "\\boxed{\\theta.grad = tensor([12.])}\n",
    "\\]\n",
    "\n",
    "이는 PyTorch가 **각 배치별 gradient를 자동으로 더해주는 누적 기능** 덕분에,  \n",
    "전체 데이터의 점수함수를 한 번에 계산할 수 있음을 보여줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aaed89",
   "metadata": {},
   "source": [
    "### Problem 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8babaca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 행 (∂f1/∂x1, ∂f1/∂x2): tensor([2.0000, 0.8776])\n",
      "두 번째 행 (∂f2/∂x1, ∂f2/∂x2): tensor([3.2183, 1.0000])\n",
      "\n",
      "최종 Jacobian 행렬:\n",
      " tensor([[2.0000, 0.8776],\n",
      "        [3.2183, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# (1) 변수 정의\n",
    "x1 = torch.tensor([1.0], requires_grad=True)\n",
    "x2 = torch.tensor([0.5], requires_grad=True)\n",
    "\n",
    "# (2) f1, f2 계산\n",
    "f1 = x1**2 + torch.sin(x2)\n",
    "f2 = x1 * x2 + torch.exp(x1)\n",
    "\n",
    "# (3) 두 출력을 하나의 벡터로 쌓기\n",
    "F = torch.stack([f1, f2])\n",
    "\n",
    "# (4) 첫 번째 출력 f1에 대해 backward() 수행 → J의 첫 번째 행 계산\n",
    "F[0].backward(retain_graph=True)  # 그래프 유지 (두 번째 backward를 위해)\n",
    "row1 = torch.tensor([x1.grad.item(), x2.grad.item()])\n",
    "\n",
    "print(\"첫 번째 행 (∂f1/∂x1, ∂f1/∂x2):\", row1)\n",
    "\n",
    "# (5) gradient 초기화\n",
    "x1.grad.zero_()\n",
    "x2.grad.zero_()\n",
    "\n",
    "# (6) 두 번째 출력 f2에 대해 backward() 수행 → J의 두 번째 행 계산\n",
    "F[1].backward()\n",
    "row2 = torch.tensor([x1.grad.item(), x2.grad.item()])\n",
    "\n",
    "print(\"두 번째 행 (∂f2/∂x1, ∂f2/∂x2):\", row2)\n",
    "\n",
    "# (7) 두 행을 합쳐서 최종 Jacobian 구성\n",
    "J = torch.stack([row1, row2])\n",
    "print(\"\\n최종 Jacobian 행렬:\\n\", J)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a42cac",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "| 단계 | 코드 | 설명 |\n",
    "|------|------|------|\n",
    "| (1) | `x1, x2`를 `requires_grad=True`로 정의 | 자동미분 가능한 변수 생성 |\n",
    "| (2) | `f1 = x1**2 + sin(x2)` <br> `f2 = x1*x2 + exp(x1)` | 함수 정의 |\n",
    "| (3) | `F = torch.stack([f1, f2])` | 두 출력을 하나의 텐서로 묶음 |\n",
    "| (4) | `F[0].backward(retain_graph=True)` | 첫 번째 출력에 대해 미분 수행 |\n",
    "| (5) | `x1.grad.zero_(), x2.grad.zero_()` | 기존 gradient 초기화 |\n",
    "| (6) | `F[1].backward()` | 두 번째 출력에 대해 미분 수행 |\n",
    "| (7) | `torch.stack([row1, row2])` | 두 행을 쌓아 최종 Jacobian 생성 |\n",
    "\n",
    "---\n",
    "\n",
    "#### 2️⃣ 이론적 계산 확인\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "f_1(x_1, x_2) &= x_1^2 + \\sin(x_2) \\\\\n",
    "f_2(x_1, x_2) &= x_1 x_2 + e^{x_1}\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "따라서,\n",
    "\\[\n",
    "J_F(x_1, x_2) =\n",
    "\\begin{bmatrix}\n",
    "2x_1 & \\cos(x_2) \\\\\n",
    "x_2 + e^{x_1} & x_1\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "x_1 = 1.0, \\quad x_2 = 0.5\n",
    "\\]\n",
    "를 대입하면,\n",
    "\n",
    "\\[\n",
    "J_F(1, 0.5) =\n",
    "\\begin{bmatrix}\n",
    "2(1) & \\cos(0.5) \\\\\n",
    "0.5 + e^{1} & 1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2.0 & 0.8776 \\\\\n",
    "3.2183 & 1.0\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "#### 3️⃣ PyTorch 계산 결과 확인\n",
    "\n",
    "실행 결과 예시는 다음과 같습니다.\n",
    "\n",
    "첫 번째 행 (∂f1/∂x1, ∂f1/∂x2): tensor([2.0000, 0.8776])\n",
    "두 번째 행 (∂f2/∂x1, ∂f2/∂x2): tensor([3.2183, 1.0000])\n",
    "\n",
    "최종 Jacobian 행렬:\n",
    "tensor([[2.0000, 0.8776],\n",
    "[3.2183, 1.0000]])\n",
    "\n",
    "\n",
    "이는 이론적 결과와 완전히 일치합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03cf7b",
   "metadata": {},
   "source": [
    "### Problem 8 https://youtu.be/kRkHcgeVAsg?si=zU3BXCxJ6PQi5Zs- 영상 6:06~8:53\n",
    "\n",
    "정재를 포함한 11명의 참가자가 노래에 맞춰 무작위로 돌다가  \n",
    "노래가 멈추면 **2명씩 짝을 지어**, **한 명이 남으면 탈락**하는 게임이다.  \n",
    "\n",
    "이 확률을 컴퓨터 시뮬레이션으로 확인하기 위해 아래 코드를 완성하시오.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3a0343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시뮬레이션 생존 확률: 0.9086\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 11명의 참가자 (0~10번), 정재는 0번\n",
    "N = 11\n",
    "num_trials = 10000\n",
    "participants = torch.arange(N)\n",
    "ds = TensorDataset(participants)\n",
    "success = 0\n",
    "\n",
    "for t in range(num_trials):\n",
    "    # 🔹 (정답) DataLoader 생성: 참가자 순서를 랜덤으로 섞고(batch_size=2)\n",
    "    loader = DataLoader(ds, batch_size=2, shuffle=True, drop_last=False)\n",
    "    survived = True\n",
    "\n",
    "    for (batch,) in loader:\n",
    "        # 한 명만 남으면 탈락\n",
    "        if batch.shape[0] == 1:\n",
    "            # 마지막 남은 사람이 정재(0)면 탈락 처리\n",
    "            if batch.item() == 0:\n",
    "                survived = False\n",
    "\n",
    "    if survived:\n",
    "        success += 1\n",
    "\n",
    "print(\"시뮬레이션 생존 확률:\", success / num_trials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea7e1dc",
   "metadata": {},
   "source": [
    "### 정답:\n",
    "DataLoader(ds, batch_size=2, shuffle=True, drop_last=False)\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **`participants = torch.arange(N)`**\n",
    "   → 참가자 번호 `[0,1,2,...,10]`을 생성합니다.  \n",
    "   여기서 **0번이 정재**입니다.\n",
    "\n",
    "2. **`TensorDataset(participants)`**\n",
    "   → 참가자 번호를 하나의 텐서 데이터셋으로 만듭니다.  \n",
    "   (DataLoader는 Dataset 형태를 입력으로 받음)\n",
    "\n",
    "3. **`DataLoader(ds, batch_size=2, shuffle=True)`**\n",
    "   → DataLoader는 매 trial마다 11명을 **랜덤하게 섞고**,  \n",
    "     2명씩 짝을 짓습니다.  \n",
    "     단, `drop_last=False`이므로 11명이 홀수일 때 마지막 한 명은 그대로 남습니다.\n",
    "\n",
    "4. **내부 for문 (`for (batch,) in loader:`)**\n",
    "   - 각 `batch`는 두 명의 참가자 (예: `[7, 4]`, `[3, 1]`, ...)  \n",
    "   - 만약 `batch.shape[0] == 1`이면, 마지막에 **혼자 남은 사람**입니다.  \n",
    "   - 그 사람이 `0`이면 → 정재 탈락(`survived=False`).\n",
    "\n",
    "5. **`if survived:`**\n",
    "   → 정재가 탈락하지 않았으면 생존 횟수(`success`)를 1 증가시킵니다.\n",
    "\n",
    "6. **`success / num_trials`**\n",
    "   → 전체 시뮬레이션 중 생존한 비율을 계산합니다.\n",
    "\n",
    "### 요약\n",
    "\n",
    "| 항목 | 내용 |\n",
    "|------|------|\n",
    "| 실험 목적 | PyTorch DataLoader의 `shuffle`과 `batch_size`를 이용해 랜덤 짝짓기 시뮬레이션 |\n",
    "| 핵심 아이디어 | 정재가 마지막 한 명으로 남는 경우만 탈락 |\n",
    "| 이론적 확률 | \\(10/11 ≈ 0.9091\\) |\n",
    "| 시뮬레이션 결과 | 약 0.909 근처 |\n",
    "| 개념 포인트 | `DataLoader`의 무작위 샘플링과 gradient 누적이 아닌 반복 샘플링 개념 이해 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f26ffc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "25_Statistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
