{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Adw-5Rx02ohB",
   "metadata": {
    "id": "Adw-5Rx02ohB"
   },
   "source": [
    "# Pytorch Lab: GLM\n",
    "# HW 7\n",
    "## 학과: 경영학부\n",
    "## 학번: 1985084\n",
    "## 이름: 임수지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c8115",
   "metadata": {},
   "source": [
    "# 4 More HW problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3599e59",
   "metadata": {},
   "source": [
    "## 4.1 HW problem \n",
    "\n",
    "\n",
    "We consider the Boston housing data, where the response variable is the median house value `medv` (in \\$1000s). We want to model `medv` using the predictors `lstat` (lower status of the population, in %) and `age` (proportion of units built prior to 1940).\n",
    "\n",
    "Train two models and compare the test mean squared error (MSE) of the following two models.\n",
    "\n",
    "**Model 1 (two predictors)**  \n",
    "We fit the linear regression model\n",
    "$$\n",
    "\\texttt{medv}_i \\sim N(\\beta_0 + \\beta_1\\,\\texttt{lstat}_i + \\beta_2\\,\\texttt{age}_i,\\, \\sigma^2),\n",
    "\\qquad i = 1, \\ldots, n.\n",
    "$$\n",
    "\n",
    "**Model 2 (one predictor)**  \n",
    "We fit the reduced model that uses `lstat` only:\n",
    "$$\n",
    "\\texttt{medv}_i \\sim N(\\beta_0 + \\beta_1\\,\\texttt{lstat}_i,\\, \\sigma^2),\n",
    "\\qquad i = 1, \\ldots, n.\n",
    "$$\n",
    "\n",
    "Using the following custom layer \n",
    "```\n",
    "class GaussianRegModel(torch.nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        self.betas = \n",
    "        self.sigma = \n",
    "\n",
    "    def forward(self, X):\n",
    "        mu = \n",
    "        return mu\n",
    "\n",
    "    def loss(self, mu, y):\n",
    "        ll = \n",
    "        nll = -torch.sum(ll)\n",
    "        return nll\n",
    "\n",
    "forward1 = GaussianRegModel(3)\n",
    "forward2 = GaussianRegModel(2)\n",
    "```\n",
    "\n",
    "Hint: \n",
    "1. You first want to make dataset(s) and dataloader(s) first.\n",
    "2. Use the below code to have the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2b4d169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X: torch.Size([354, 12])\n",
      "Train y: torch.Size([354, 1])\n",
      "Test X: torch.Size([152, 12])\n",
      "Test y: torch.Size([152, 1])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. Load CSV\n",
    "df = pd.read_csv(\"Boston.csv\")\n",
    "# 보스턴 지역을 몇백 개로 나누고, 그 지역에서 사는 사람들의 평균 나이, 기초수급자 비율 두 가지 설명변수로, 반응 변수는 집값으로 하는 데이터.\n",
    "# 집값이 비싼지 안 비싼지 분석하는 normal regression 문제.\n",
    "\n",
    "# 2. Convert to tensor\n",
    "data = torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "# 3. Shuffle rows manually\n",
    "n = data.shape[0]\n",
    "perm = torch.randperm(n)\n",
    "\n",
    "shuffled = data[perm]\n",
    "\n",
    "# 4. Train/Test split (70% train, 30% test)\n",
    "n_train = int(n * 0.7)\n",
    "\n",
    "train = shuffled[:n_train]\n",
    "test  = shuffled[n_train:]\n",
    "\n",
    "# 5. Split into X and y\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1:]\n",
    "\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1:]\n",
    "\n",
    "print(\"Train X:\", X_train.shape)\n",
    "print(\"Train y:\", y_train.shape)\n",
    "print(\"Test X:\", X_test.shape)\n",
    "print(\"Test y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ec3ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianRegModel(torch.nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        self.betas = torch.nn.Parameter(torch.randn([p, 1]))\n",
    "        self.sigma = torch.nn.Parameter(torch.abs(torch.randn([1]))) # log(sigma) to ensure sigma > 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        mu = X @ self.betas\n",
    "        return mu\n",
    "\n",
    "    def loss(self, mu, y):\n",
    "        ll = -1/2*torch.log(2*torch.pi*self.sigma**2) - (y - mu)**2/(2*self.sigma**2)\n",
    "        nll = -torch.sum(ll)\n",
    "        return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e35cfb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "ones = torch.ones(X_train.shape[0])\n",
    "XX_train = torch.stack([ones, X_train[:, 6], X_train[:, 11]], axis=1)\n",
    "ones = torch.ones(X_test.shape[0])\n",
    "XX_test = torch.stack([ones, X_test[:, 6], X_test[:, 11]], axis=1)\n",
    "\n",
    "train = TensorDataset(XX_train, y_train)\n",
    "train_loader = DataLoader(train, shuffle=True, batch_size=20)\n",
    "\n",
    "test = TensorDataset(XX_test, y_test)\n",
    "test_loader = DataLoader(test, shuffle=False, batch_size=20)\n",
    "# 왜 train은 셔플하고 test는 셔플 안 했는지\n",
    "# 핵심: Train은 \"학습 효과\"를 위해, Test는 \"평가의 일관성\"을 위해."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac15156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화!!\n",
    "X_train_mean = torch.mean(X_train[:, [6,11]], axis=0)\n",
    "X_train_std = torch.std(X_train[:, [6,11]], axis=0)\n",
    "XXX_train = (X_train[:, [6,11]] - X_train_mean) / X_train_std\n",
    "ones = torch.ones(X_train.shape[0])\n",
    "XXX_train = torch.concat([ones.reshape([-1,1]), XXX_train], axis=1)\n",
    "ones = torch.ones(X_test.shape[0])\n",
    "XXX_test = (X_test[:, [6,11]] - X_train_mean) / X_train_std\n",
    "XXX_test = torch.concat([ones.reshape([-1,1]), XXX_test], axis=1)\n",
    "\n",
    "train = TensorDataset(XX_train, y_train)\n",
    "train_loader = DataLoader(train, shuffle=True, batch_size=20)\n",
    "\n",
    "test = TensorDataset(XXX_test, y_test)\n",
    "test_loader = DataLoader(test, shuffle=False, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32801952",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward1 = GaussianRegModel(3)\n",
    "forward2 = GaussianRegModel(2)\n",
    "\n",
    "lr=0.01\n",
    "optimizer = torch.optim.SGD(forward1.parameters(), lr=lr)\n",
    "\n",
    "history=[]\n",
    "\n",
    "for i in range(500):\n",
    "    LOSS=0\n",
    "    for xx, yy in train_loader: # xx:[-1, 3] yy:[-1,1]\n",
    "      uhat = forward1(xx) # [-1,1]\n",
    "      loss = forward1.loss(uhat, yy)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "      LOSS+=loss.item()\n",
    "    history.append(LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e03364d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28f0b6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(727.1813, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test MSE\n",
    "S=0\n",
    "for xx, yy in test_loader:\n",
    "    uhat = forward1(xx)\n",
    "    sum_loss = torch.sum((yy - uhat)**2)\n",
    "    S += sum_loss\n",
    "S/len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ab8fd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'betas': tensor([[  1.2416],\n",
      "        [  4.0074],\n",
      "        [-21.0629]]), 'sigma': tensor([619.2971])})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nyhat = b0 + b1 * x1 + b2 * x2 # 이런 식으로 하면 안 됨! 정규화를 했기 때문.\\nyhat = b0 + b1 * (age - 67.6)/28.3 + b2*(lstat-12.7)/7.12 # 이런 식임.\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction\n",
    "\n",
    "# 추정된 b를 보자\n",
    "print(forward1.state_dict())\n",
    "\"\"\"\n",
    "yhat = b0 + b1 * x1 + b2 * x2 # 이런 식으로 하면 안 됨! 정규화를 했기 때문.\n",
    "yhat = b0 + b1 * (age - 67.6)/28.3 + b2*(lstat-12.7)/7.12 # 이런 식임.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d0ede4",
   "metadata": {},
   "source": [
    "# 4.2 HW problem\n",
    "We consider the built-in dataset **`warpbreaks`**, which describes the effect of  \n",
    "- wool **type** (`A` or `B`) and  \n",
    "- **tension** (`low`, `medium`, or `high`)  \n",
    "\n",
    "on the number of warp breaks per loom.  \n",
    "The response variable is **`breaks`**, a count of the number of breaks.  \n",
    "The predictor variables are **`type`** and **`tension`**.\n",
    "\n",
    "Our goal is to compare the following three Poisson models (do not forget to have exponantial activation function):\n",
    "\n",
    "- **Model 1:**  \n",
    "  `breaks ~ type`\n",
    "\n",
    "- **Model 2:**  \n",
    "  `breaks ~ tension`\n",
    "\n",
    "- **Model 3:**  \n",
    "  `breaks ~ type + tension`\n",
    "\n",
    "---\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. **Estimate the parameters using MLE (Maximum Likelihood Estimation)** for each of the three models (use custom layer with both forward and loss functions in it).\n",
    "\n",
    "2. **Compute train AIC** for each model based on your parameter estimates (using training data only, do not use test data).  \n",
    "   Which model is better in terms of AIC?\n",
    "\n",
    "### Hint 1:\n",
    "\n",
    "1. You want use the below code to extract the data.\n",
    "2. Make dataset and dataloader first.\n",
    "3. Since the predictors are categorical, I have converted them into dummy variables before feeding them into your model. See Hint 2 and 3 to see how this dummy variables work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e301a33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['breaks', 'wool', 'tension']\n",
      "First 3 rows: [['26', 'A', 'L'], ['30', 'A', 'L'], ['54', 'A', 'L']]\n",
      "Full tensor shape: torch.Size([54, 5])\n",
      "tensor([[26.,  1.,  0.,  0.,  0.],\n",
      "        [30.,  1.,  0.,  0.,  0.],\n",
      "        [54.,  1.,  0.,  0.,  0.],\n",
      "        [25.,  1.,  0.,  0.,  0.],\n",
      "        [70.,  1.,  0.,  0.,  0.]])\n",
      "Train X: torch.Size([37, 4])\n",
      "Train y: torch.Size([37, 1])\n",
      "Test X: torch.Size([17, 4])\n",
      "Test y: torch.Size([17, 1])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Load CSV manually\n",
    "# -------------------------------------------------------\n",
    "filename = \"warpbreaks.csv\"\n",
    "\n",
    "rows = []\n",
    "with open(filename, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)   # breaks, wool, tension\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "print(\"Header:\", header)\n",
    "print(\"First 3 rows:\", rows[:3])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Prepare lists for each variable\n",
    "# -------------------------------------------------------\n",
    "breaks_list = []\n",
    "wool_list   = []\n",
    "tension_list = []\n",
    "\n",
    "for row in rows:\n",
    "    b, w, t = row\n",
    "\n",
    "    breaks_list.append(float(b))\n",
    "    wool_list.append(w)\n",
    "    tension_list.append(t)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Manual dummy encoding\n",
    "# -------------------------------------------------------\n",
    "tension_map = {\n",
    "    \"L\": (0.0, 0.0),\n",
    "    \"M\": (1.0, 0.0),\n",
    "    \"H\": (0.0, 1.0)\n",
    "}\n",
    "\n",
    "wool_map = {\n",
    "    \"A\": 0.0,\n",
    "    \"B\": 1.0\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Build full matrix: [breaks, intercept, X1, X2, X3]\n",
    "#    where intercept = 1.0\n",
    "# -------------------------------------------------------\n",
    "data_matrix = []\n",
    "\n",
    "for i in range(len(breaks_list)):\n",
    "    t1, t2 = tension_map[tension_list[i]]\n",
    "    x3 = wool_map[wool_list[i]]\n",
    "\n",
    "    row = [\n",
    "        breaks_list[i],  # y\n",
    "        1.0,             # intercept\n",
    "        t1,              # X1 = tension_M\n",
    "        t2,              # X2 = tension_H\n",
    "        x3               # X3 = wool_B\n",
    "    ]\n",
    "    data_matrix.append(row)\n",
    "\n",
    "data = torch.tensor(data_matrix, dtype=torch.float32)\n",
    "\n",
    "print(\"Full tensor shape:\", data.shape)\n",
    "print(data[:5])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Shuffle the entire tensor\n",
    "# -------------------------------------------------------\n",
    "n = data.shape[0]\n",
    "perm = torch.randperm(n)\n",
    "shuffled = data[perm]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. 70% train / 30% test split\n",
    "# -------------------------------------------------------\n",
    "n_train = int(n * 0.7)\n",
    "\n",
    "train = shuffled[:n_train]\n",
    "test  = shuffled[n_train:]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7. Split into X and y\n",
    "# -------------------------------------------------------\n",
    "y_train = train[:, 0:1]      # breaks\n",
    "X_train = train[:, 1:]       # intercept + X1 + X2 + X3\n",
    "\n",
    "y_test = test[:, 0:1]\n",
    "X_test = test[:, 1:]\n",
    "\n",
    "print(\"Train X:\", X_train.shape)  # should be (n_train, 4)\n",
    "print(\"Train y:\", y_train.shape)\n",
    "print(\"Test X:\",  X_test.shape)   # should be (n_test, 4)\n",
    "print(\"Test y:\",  y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa227d",
   "metadata": {},
   "source": [
    "### Hint 2: Dummy Encoding\n",
    "\n",
    "Categorical variables such as `\"wool\"` (A/B) or `\"tension\"` (L/M/H) cannot be used directly in most statistical or machine learning models. To include them as predictors, we must convert them into numerical form.\n",
    "\n",
    "One commonly used approach is **dummy encoding**.  \n",
    "In dummy encoding:\n",
    "\n",
    "- A categorical variable with **K categories** is represented using **K−1 binary indicator variables**.\n",
    "- One category is treated as the **baseline**, and the remaining categories are encoded with dummy variables that take the value 0 or 1.\n",
    "\n",
    "This creates numerical predictor columns while avoiding the multicollinearity problem that occurs with full one-hot encoding.\n",
    "\n",
    "For example, suppose we have a variable with three categories:\n",
    "\n",
    "- Red  \n",
    "- Green  \n",
    "- Blue  \n",
    "\n",
    "We can encode these using **two** dummy variables:\n",
    "\n",
    "| Category | Encoded value (X1, X2) |\n",
    "|----------|------------------------|\n",
    "| Red      | (0, 0)                |\n",
    "| Green    | (1, 0)                |\n",
    "| Blue     | (0, 1)                |\n",
    "\n",
    "Here:\n",
    "\n",
    "- **Red** is the baseline category  \n",
    "- **X1** represents Green  \n",
    "- **X2** represents Blue  \n",
    "- The intercept absorbs the baseline effect\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "### Toy Example in PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Suppose the categorical variable has 3 levels:\n",
    "# 0 = Red, 1 = Green, 2 = Blue\n",
    "cats = torch.tensor([2, 0, 1, 2], dtype=torch.long)\n",
    "\n",
    "# We will create two dummy variables (X1, X2)\n",
    "# Baseline category: Red → (0,0)\n",
    "X = torch.zeros((len(cats), 2))\n",
    "\n",
    "for i, c in enumerate(cats):\n",
    "    if c == 1:          # Green\n",
    "        X[i, 0] = 1\n",
    "    elif c == 2:        # Blue\n",
    "        X[i, 1] = 1\n",
    "    # If c == 0 (Red), leave as (0,0)\n",
    "\n",
    "print(\"Dummy-encoded matrix:\")\n",
    "print(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96c500b",
   "metadata": {},
   "source": [
    "### Hint 3: Back to our problem\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We have\n",
    "\n",
    "\n",
    "| tension | dummy encoding (X1, X2) |\n",
    "|---------|--------------------------|\n",
    "| L       | (0, 0)                  |\n",
    "| M       | (1, 0)                  |\n",
    "| H       | (0, 1)                  |\n",
    "\n",
    "| wool | dummy encoding (X3) |\n",
    "|------|----------------------|\n",
    "| A    | 0                    |\n",
    "| B    | 1                    |\n",
    "\n",
    "\n",
    "Now you have columns for `X1`, `X2`, and `X3` as well as intercept column. Hence your matrix `X` for the explanatory variable is a shape of `[n, 4]`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f0a3c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a885e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Poisson Regression Model\n",
    "class PoissonRegModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.betas = nn.Parameter(torch.randn([p, 1]) * 0.01)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Poisson regression uses exponential activation\n",
    "        lam = torch.exp(X @ self.betas)\n",
    "        return lam\n",
    "    \n",
    "    def loss(self, lam, y):\n",
    "        # Poisson negative log-likelihood\n",
    "        # -log P(y|lambda) = lambda - y*log(lambda) + log(y!)\n",
    "        # We ignore log(y!) as it doesn't depend on parameters\n",
    "        ll = y * torch.log(lam + 1e-8) - lam\n",
    "        nll = -torch.sum(ll)\n",
    "        return nll\n",
    "\n",
    "\n",
    "# Prepare DataLoaders for training (we won't use test data for AIC)\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=20)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Model 1: breaks ~ type (X3 only)\n",
    "# ========================================\n",
    "print(\"=\"*60)\n",
    "print(\"Training Model 1: breaks ~ type\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# X for Model 1: intercept + X3 (wool_B)\n",
    "X1_train = X_train[:, [0, 3]]  # columns: intercept, X3\n",
    "\n",
    "model1 = PoissonRegModel(2)  # 2 parameters\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.01)\n",
    "\n",
    "train1_loader = DataLoader(TensorDataset(X1_train, y_train), shuffle=True, batch_size=20)\n",
    "\n",
    "history1 = []\n",
    "for epoch in range(500):\n",
    "    total_loss = 0\n",
    "    for xx, yy in train1_loader:\n",
    "        lam = model1(xx)\n",
    "        loss = model1.loss(lam, yy)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer1.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    history1.append(total_loss)\n",
    "\n",
    "print(f\"Final loss: {history1[-1]:.4f}\")\n",
    "print(f\"Parameters: {model1.betas.data.T}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Model 2: breaks ~ tension (X1, X2)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Model 2: breaks ~ tension\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# X for Model 2: intercept + X1 + X2 (tension_M, tension_H)\n",
    "X2_train = X_train[:, [0, 1, 2]]  # columns: intercept, X1, X2\n",
    "\n",
    "model2 = PoissonRegModel(3)  # 3 parameters\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.01)\n",
    "\n",
    "train2_loader = DataLoader(TensorDataset(X2_train, y_train), shuffle=True, batch_size=20)\n",
    "\n",
    "history2 = []\n",
    "for epoch in range(500):\n",
    "    total_loss = 0\n",
    "    for xx, yy in train2_loader:\n",
    "        lam = model2(xx)\n",
    "        loss = model2.loss(lam, yy)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        optimizer2.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    history2.append(total_loss)\n",
    "\n",
    "print(f\"Final loss: {history2[-1]:.4f}\")\n",
    "print(f\"Parameters: {model2.betas.data.T}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Model 3: breaks ~ type + tension (all)\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Model 3: breaks ~ type + tension\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# X for Model 3: all columns (intercept + X1 + X2 + X3)\n",
    "X3_train = X_train  # all 4 columns\n",
    "\n",
    "model3 = PoissonRegModel(4)  # 4 parameters\n",
    "optimizer3 = torch.optim.SGD(model3.parameters(), lr=0.01)\n",
    "\n",
    "train3_loader = DataLoader(TensorDataset(X3_train, y_train), shuffle=True, batch_size=20)\n",
    "\n",
    "history3 = []\n",
    "for epoch in range(500):\n",
    "    total_loss = 0\n",
    "    for xx, yy in train3_loader:\n",
    "        lam = model3(xx)\n",
    "        loss = model3.loss(lam, yy)\n",
    "        loss.backward()\n",
    "        optimizer3.step()\n",
    "        optimizer3.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    history3.append(total_loss)\n",
    "\n",
    "print(f\"Final loss: {history3[-1]:.4f}\")\n",
    "print(f\"Parameters: {model3.betas.data.T}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Plot training curves\n",
    "# ========================================\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history1)\n",
    "plt.title('Model 1: breaks ~ type')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history2)\n",
    "plt.title('Model 2: breaks ~ tension')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history3)\n",
    "plt.title('Model 3: breaks ~ type + tension')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Compute Train AIC for each model\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AIC Comparison (using training data)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def compute_train_aic(model, X_data, y_data):\n",
    "    \"\"\"\n",
    "    AIC = 2k - 2*log(L)\n",
    "    where k = number of parameters\n",
    "          L = likelihood\n",
    "    \n",
    "    For Poisson: log(L) = sum(y*log(lambda) - lambda - log(y!))\n",
    "    We can ignore log(y!) as it's constant across models\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        lam = model(X_data)\n",
    "        # Compute log-likelihood\n",
    "        log_likelihood = torch.sum(y_data * torch.log(lam + 1e-8) - lam).item()\n",
    "        \n",
    "        # Number of parameters\n",
    "        k = model.betas.numel()\n",
    "        \n",
    "        # AIC = 2k - 2*log(L)\n",
    "        aic = 2 * k - 2 * log_likelihood\n",
    "    \n",
    "    return aic, log_likelihood, k\n",
    "\n",
    "aic1, ll1, k1 = compute_train_aic(model1, X1_train, y_train)\n",
    "aic2, ll2, k2 = compute_train_aic(model2, X2_train, y_train)\n",
    "aic3, ll3, k3 = compute_train_aic(model3, X3_train, y_train)\n",
    "\n",
    "print(f\"\\nModel 1 (breaks ~ type):\")\n",
    "print(f\"  Parameters (k): {k1}\")\n",
    "print(f\"  Log-Likelihood: {ll1:.4f}\")\n",
    "print(f\"  AIC: {aic1:.4f}\")\n",
    "\n",
    "print(f\"\\nModel 2 (breaks ~ tension):\")\n",
    "print(f\"  Parameters (k): {k2}\")\n",
    "print(f\"  Log-Likelihood: {ll2:.4f}\")\n",
    "print(f\"  AIC: {aic2:.4f}\")\n",
    "\n",
    "print(f\"\\nModel 3 (breaks ~ type + tension):\")\n",
    "print(f\"  Parameters (k): {k3}\")\n",
    "print(f\"  Log-Likelihood: {ll3:.4f}\")\n",
    "print(f\"  AIC: {aic3:.4f}\")\n",
    "\n",
    "# Find best model (lowest AIC)\n",
    "aics = {'Model 1': aic1, 'Model 2': aic2, 'Model 3': aic3}\n",
    "best_model = min(aics, key=aics.get)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"✓ Best Model: {best_model} (lowest AIC = {aics[best_model]:.4f})\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a500881",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce129598",
   "metadata": {},
   "source": [
    "## 4.3 HW problem\n",
    "\n",
    "You are given the Default data, and want to predict one will default(`y=1`) or not(`y=0`). You have explanatory variable `student`, `balance`, and `income`. You will make two models:\n",
    "\n",
    "1. Model 1: Use `student` and `income` to predict the `default`.\n",
    "2. Model 2: Use `student`, `income` and `balance` to predict the `default`.\n",
    "\n",
    "Compare the two models in terms of **test accuracy**. Which is better? (use custom layer with `forward` method and `loss` method in it.)\n",
    "\n",
    "**Note:** You only need to compute and compare test accuracy for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7087e2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: ['default', 'student', 'balance', 'income']\n",
      "First 3 rows: [['No', 'No', '729.526495207286', '44361.6250742669'], ['No', 'Yes', '817.180406555498', '12106.1347003149'], ['No', 'No', '1073.54916401173', '31767.1389473999']]\n",
      "Full tensor shape: torch.Size([10000, 4])\n",
      "tensor([[7.2953e+02, 4.4362e+04, 0.0000e+00, 0.0000e+00],\n",
      "        [8.1718e+02, 1.2106e+04, 1.0000e+00, 0.0000e+00],\n",
      "        [1.0735e+03, 3.1767e+04, 0.0000e+00, 0.0000e+00]])\n",
      "Train X: torch.Size([7000, 3])\n",
      "Train y: torch.Size([7000, 1])\n",
      "Test X: torch.Size([3000, 3])\n",
      "Test y: torch.Size([3000, 1])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Load CSV file manually (no pandas)\n",
    "# -------------------------------------------------------\n",
    "filename = \"Default_data.csv\"\n",
    "\n",
    "rows = []\n",
    "with open(filename, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    header = next(reader)   # skip header\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "\n",
    "print(\"Header:\", header)\n",
    "print(\"First 3 rows:\", rows[:3])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Convert categorical + numeric values\n",
    "# -------------------------------------------------------\n",
    "default_list = []\n",
    "student_list = []\n",
    "balance_list = []\n",
    "income_list = []\n",
    "\n",
    "for row in rows:\n",
    "    default_str, student_str, balance_str, income_str = row\n",
    "\n",
    "    # Yes/No → 1/0\n",
    "    default = 1.0 if default_str.lower() == \"yes\" else 0.0\n",
    "    student = 1.0 if student_str.lower() == \"yes\" else 0.0\n",
    "\n",
    "    default_list.append(default)\n",
    "    student_list.append(student)\n",
    "    balance_list.append(float(balance_str))\n",
    "    income_list.append(float(income_str))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3. Make a SINGLE tensor (X + y together)\n",
    "#    Column order: balance, income, student, default\n",
    "# -------------------------------------------------------\n",
    "data_matrix = []\n",
    "for i in range(len(default_list)):\n",
    "    row = [\n",
    "        balance_list[i],\n",
    "        income_list[i],\n",
    "        student_list[i],\n",
    "        default_list[i]   # last column is y\n",
    "    ]\n",
    "    data_matrix.append(row)\n",
    "\n",
    "data = torch.tensor(data_matrix, dtype=torch.float32)\n",
    "\n",
    "print(\"Full tensor shape:\", data.shape)\n",
    "print(data[:3])\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4. Shuffle entire dataset\n",
    "# -------------------------------------------------------\n",
    "n = data.shape[0]\n",
    "perm = torch.randperm(n)\n",
    "shuffled = data[perm]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5. Train/Test split (70/30)\n",
    "# -------------------------------------------------------\n",
    "n_train = int(n * 0.7)\n",
    "\n",
    "train = shuffled[:n_train]\n",
    "test  = shuffled[n_train:]\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6. Split into X and y\n",
    "# -------------------------------------------------------\n",
    "X_train = train[:, :-1]   # all columns except last\n",
    "y_train = train[:, -1:]   # last column only\n",
    "\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1:]\n",
    "\n",
    "print(\"Train X:\", X_train.shape)\n",
    "print(\"Train y:\", y_train.shape)\n",
    "print(\"Test X:\", X_test.shape)\n",
    "print(\"Test y:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Logistic Regression Model\n",
    "class LogisticRegModel(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super().__init__()\n",
    "        self.betas = nn.Parameter(torch.randn([p, 1]) * 0.01)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Logistic regression uses sigmoid activation\n",
    "        logits = X @ self.betas\n",
    "        pp = torch.sigmoid(logits)\n",
    "        return pp\n",
    "    \n",
    "    def loss(self, pp, y):\n",
    "        # Binary cross-entropy (Bernoulli negative log-likelihood)\n",
    "        # -log P(y|p) = -[y*log(p) + (1-y)*log(1-p)]\n",
    "        ll = y * torch.log(pp + 1e-8) + (1 - y) * torch.log(1 - pp + 1e-8)\n",
    "        nll = -torch.sum(ll)\n",
    "        return nll\n",
    "\n",
    "\n",
    "# Data is already loaded: X_train, y_train, X_test, y_test\n",
    "# Columns: [balance, income, student, default]\n",
    "# We need to add intercept and select appropriate columns\n",
    "\n",
    "# Normalize features for better training\n",
    "balance_mean = X_train[:, 0].mean()\n",
    "balance_std = X_train[:, 0].std()\n",
    "income_mean = X_train[:, 1].mean()\n",
    "income_std = X_train[:, 1].std()\n",
    "\n",
    "# Normalize training data\n",
    "X_train_norm = X_train.clone()\n",
    "X_train_norm[:, 0] = (X_train[:, 0] - balance_mean) / balance_std\n",
    "X_train_norm[:, 1] = (X_train[:, 1] - income_mean) / income_std\n",
    "\n",
    "# Normalize test data (using training statistics)\n",
    "X_test_norm = X_test.clone()\n",
    "X_test_norm[:, 0] = (X_test[:, 0] - balance_mean) / balance_std\n",
    "X_test_norm[:, 1] = (X_test[:, 1] - income_mean) / income_std\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Model 1: default ~ student + income\n",
    "# ========================================\n",
    "print(\"=\"*60)\n",
    "print(\"Training Model 1: default ~ student + income\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data: intercept + income + student\n",
    "ones_train = torch.ones([X_train.shape[0], 1])\n",
    "X1_train = torch.cat([ones_train, X_train_norm[:, 1:2], X_train_norm[:, 2:3]], dim=1)  # intercept, income, student\n",
    "\n",
    "ones_test = torch.ones([X_test.shape[0], 1])\n",
    "X1_test = torch.cat([ones_test, X_test_norm[:, 1:2], X_test_norm[:, 2:3]], dim=1)\n",
    "\n",
    "# Create DataLoaders\n",
    "train1_dataset = TensorDataset(X1_train, y_train)\n",
    "train1_loader = DataLoader(train1_dataset, shuffle=True, batch_size=50)\n",
    "\n",
    "test1_dataset = TensorDataset(X1_test, y_test)\n",
    "test1_loader = DataLoader(test1_dataset, shuffle=False, batch_size=50)\n",
    "\n",
    "# Train Model 1\n",
    "model1 = LogisticRegModel(3)  # 3 parameters: intercept + income + student\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=0.1)\n",
    "\n",
    "history1 = []\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    for xx, yy in train1_loader:\n",
    "        pp = model1(xx)\n",
    "        loss = model1.loss(pp, yy)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        optimizer1.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    history1.append(total_loss)\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Training Loss: {history1[-1]:.4f}\")\n",
    "print(f\"Parameters: {model1.betas.data.T}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Model 2: default ~ student + income + balance\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Model 2: default ~ student + income + balance\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data: intercept + balance + income + student\n",
    "X2_train = torch.cat([ones_train, X_train_norm], dim=1)  # intercept + balance + income + student\n",
    "X2_test = torch.cat([ones_test, X_test_norm], dim=1)\n",
    "\n",
    "# Create DataLoaders\n",
    "train2_dataset = TensorDataset(X2_train, y_train)\n",
    "train2_loader = DataLoader(train2_dataset, shuffle=True, batch_size=50)\n",
    "\n",
    "test2_dataset = TensorDataset(X2_test, y_test)\n",
    "test2_loader = DataLoader(test2_dataset, shuffle=False, batch_size=50)\n",
    "\n",
    "# Train Model 2\n",
    "model2 = LogisticRegModel(4)  # 4 parameters: intercept + balance + income + student\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=0.1)\n",
    "\n",
    "history2 = []\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    for xx, yy in train2_loader:\n",
    "        pp = model2(xx)\n",
    "        loss = model2.loss(pp, yy)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        optimizer2.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    history2.append(total_loss)\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Training Loss: {history2[-1]:.4f}\")\n",
    "print(f\"Parameters: {model2.betas.data.T}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Plot training curves\n",
    "# ========================================\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history1)\n",
    "plt.title('Model 1: default ~ student + income')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history2)\n",
    "plt.title('Model 2: default ~ student + income + balance')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# Compute Test Accuracy\n",
    "# ========================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test Accuracy Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def compute_accuracy(model, data_loader):\n",
    "    \"\"\"Compute classification accuracy\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xx, yy in data_loader:\n",
    "            pp = model(xx)\n",
    "            # Convert probabilities to predictions (threshold = 0.5)\n",
    "            predictions = (pp >= 0.5).float()\n",
    "            correct += (predictions == yy).sum().item()\n",
    "            total += yy.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Compute test accuracy for both models\n",
    "acc1 = compute_accuracy(model1, test1_loader)\n",
    "acc2 = compute_accuracy(model2, test2_loader)\n",
    "\n",
    "print(f\"\\nModel 1 (student + income):\")\n",
    "print(f\"  Test Accuracy: {acc1:.4f} ({acc1*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nModel 2 (student + income + balance):\")\n",
    "print(f\"  Test Accuracy: {acc2:.4f} ({acc2*100:.2f}%)\")\n",
    "\n",
    "# Determine better model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if acc1 > acc2:\n",
    "    print(f\"✓ Model 1 is better (higher accuracy by {(acc1-acc2)*100:.2f}%)\")\n",
    "elif acc2 > acc1:\n",
    "    print(f\"✓ Model 2 is better (higher accuracy by {(acc2-acc1)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"Both models have the same accuracy\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "25_Statistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
