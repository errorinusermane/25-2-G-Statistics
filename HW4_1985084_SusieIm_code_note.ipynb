{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d42abbba",
   "metadata": {},
   "source": [
    "# HW 4\n",
    "## 학과: 경영학부\n",
    "## 학번: 1985084\n",
    "## 이름: 임수지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44796571",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f6ac5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. 파라미터 초기화\n",
    "import torch\n",
    "\n",
    "# 데이터\n",
    "X1 = torch.tensor([0.83, -0.18, 0.27, 0.32, 0.25, -0.73, 0.19, -0.08, -0.47, -0.28,\n",
    "                   -0.23, 0.25, 0.15, 0.92, 1.27, -0.50, 0.05, -0.74, -0.06, 0.70])\n",
    "X2 = torch.tensor([0.63, -1.43, -1.35, 0.70, -1.48, 0.06, 1.14, 0.96, -1.65, 0.44,\n",
    "                   0.62, 0.47, -0.08, 1.03, 1.24, -0.54, 0.08, 0.85, -1.53, 0.66])\n",
    "Y  = torch.tensor([0.50, 1.34, 1.39, 0.40, 1.20, 0.11, -0.21, -0.26, 1.18, 0.59,\n",
    "                   0.13, 0.17, 0.55, 0.32, 0.19, 0.46, 0.32, -0.35, 1.35, 0.20])\n",
    "\n",
    "# 초기값 (보통 0 혹은 작은 랜덤값)\n",
    "beta_0 = torch.randn(1, requires_grad=True)\n",
    "beta_1 = torch.randn(1, requires_grad=True)\n",
    "beta_2 = torch.randn(1, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb3a8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 | Loss = 7.4003\n",
      "Epoch  500 | Loss = 0.4702\n",
      "Epoch 1000 | Loss = 0.4702\n",
      "Epoch 1500 | Loss = 0.4702\n",
      "Epoch 2000 | Loss = 0.4702\n",
      "Epoch 2500 | Loss = 0.4702\n",
      "Epoch 3000 | Loss = 0.4702\n",
      "Epoch 3500 | Loss = 0.4702\n",
      "Epoch 4000 | Loss = 0.4702\n",
      "Epoch 4500 | Loss = 0.4702\n"
     ]
    }
   ],
   "source": [
    "# Step 2. 예측 함수 정의\n",
    "def predict(X1, X2, beta_0, beta_1, beta_2):\n",
    "    return beta_0 + beta_1 * X1 + beta_2 * X2\n",
    "\n",
    "# Step 3. 손실함수 정의 (SSE)\n",
    "def sse_loss(Y, Y_hat):\n",
    "    return torch.sum((Y - Y_hat)**2)\n",
    "\n",
    "# Step 4. 경사하강법 루프\n",
    "lr = 0.01\n",
    "n_epochs = 5000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # forward\n",
    "    Y_hat = predict(X1, X2, beta_0, beta_1, beta_2)\n",
    "    loss = sse_loss(Y, Y_hat)\n",
    "\n",
    "    # gradient 초기화\n",
    "    loss.backward()\n",
    "\n",
    "    # update (gradient descent)\n",
    "    with torch.no_grad():\n",
    "        beta_0 -= lr * beta_0.grad\n",
    "        beta_1 -= lr * beta_1.grad\n",
    "        beta_2 -= lr * beta_2.grad\n",
    "\n",
    "    # gradient 초기화\n",
    "    beta_0.grad.zero_()\n",
    "    beta_1.grad.zero_()\n",
    "    beta_2.grad.zero_()\n",
    "\n",
    "    # 몇 번마다 출력\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch:4d} | Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f4edd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0: 0.4672396779060364\n",
      "beta_1: 0.35831204056739807\n",
      "beta_2: -0.5565071702003479\n",
      "MSE: 0.023509198799729347\n"
     ]
    }
   ],
   "source": [
    "print(\"beta_0:\", beta_0.item())\n",
    "print(\"beta_1:\", beta_1.item())\n",
    "print(\"beta_2:\", beta_2.item())\n",
    "\n",
    "# 최종 예측값과 MSE 계산\n",
    "Y_pred = predict(X1, X2, beta_0, beta_1, beta_2)\n",
    "MSE = torch.mean((Y - Y_pred)**2)\n",
    "print(\"MSE:\", MSE.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c18fd4",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d9033d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(X <= 2.0) = 0.9257264137268066\n"
     ]
    }
   ],
   "source": [
    "# 2번\n",
    "rate=1.3\n",
    "torch.distributions.Exponential(rate, validate_args=None)\n",
    "\n",
    "# 3번\n",
    "import torch\n",
    "\n",
    "rate = torch.tensor([1.3])\n",
    "exp_dist = torch.distributions.Exponential(rate=rate)\n",
    "\n",
    "x = torch.tensor([2.0])\n",
    "cdf_x = exp_dist.cdf(x)\n",
    "print(\"P(X <= 2.0) =\", cdf_x.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64bb416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 (icdf) = 2.3044092655181885\n",
      "\n",
      "최종 추정값 (gradient descent): 2.304096221923828\n"
     ]
    }
   ],
   "source": [
    "# 4번\n",
    "import torch\n",
    "\n",
    "# (a) 손으로 풀기 → 1번 손풀이 참고\n",
    "# x0 = -torch.log(torch.tensor(0.05)) / 1.3 = 2.304\n",
    "\n",
    "# (b) PyTorch icdf 사용\n",
    "rate = torch.tensor([1.3])\n",
    "exp_dist = torch.distributions.Exponential(rate=rate)\n",
    "x0_icdf = exp_dist.icdf(torch.tensor([0.95]))\n",
    "print(\"x0 (icdf) =\", x0_icdf.item())  # 약 2.304 출력\n",
    "\n",
    "# (c) 경사하강법(Gradient Descent)으로 최적화\n",
    "\n",
    "# θ를 이용한 softplus 변환 (항상 양수)\n",
    "theta = torch.tensor([1.0], requires_grad=True)  # softplus(1.0) ≈ 1.313\n",
    "optimizer = torch.optim.SGD([theta], lr=0.05)\n",
    "\n",
    "for step in range(100000):\n",
    "    # softplus 변환: 항상 x0 > 0\n",
    "    x0 = torch.log1p(torch.exp(theta))\n",
    "    \n",
    "    # 손실함수: (CDF(x0) - 0.95)^2\n",
    "    loss = (exp_dist.cdf(x0) - 0.95)**2\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"\\n최종 추정값 (gradient descent):\", x0.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "25_Statistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
